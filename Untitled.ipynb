{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08da4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! HDF5 and XDMF files including data have been created.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01810e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! HDF5 and XDMF files including data have been created.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to parse the data file\n",
    "def parse_data_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    metadata = {}\n",
    "    data = {}\n",
    "    current_ts = None\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if not parts:\n",
    "            continue\n",
    "        if parts[0] == 'ND':\n",
    "            metadata['ND'] = int(parts[1])\n",
    "        elif parts[0] == 'NC':\n",
    "            metadata['NC'] = int(parts[1])\n",
    "        elif parts[0] == 'NAME':\n",
    "            metadata['NAME'] = parts[1]\n",
    "        elif parts[0] == 'TIMEUNITS':\n",
    "            metadata['TIMEUNITS'] = parts[1]\n",
    "        elif parts[0] == 'TS':\n",
    "            current_ts = float(parts[2])\n",
    "            data[current_ts] = []\n",
    "        elif parts[0] == 'ENDDS':\n",
    "            continue\n",
    "        elif current_ts is not None:\n",
    "            data[current_ts].append([float(p) for p in parts])\n",
    "    \n",
    "    return metadata, data\n",
    "\n",
    "# File paths\n",
    "name = \"Flap_Conc\"\n",
    "input_3dm_file = name + '.3dm'\n",
    "output_h5_file = name + '.h5'\n",
    "output_xdmf_file = name + '.xdmf'\n",
    "input_data_files = ['Flap_Conc_ovl.dat', 'Flap_Conc_con1.dat']  # Replace with your actual data file paths\n",
    "\n",
    "# Initialize lists to store nodes, elements, and data\n",
    "nodes = []\n",
    "elements = []\n",
    "data_dicts = {}  # Dictionary to store data lists for each timestep keyed by file base name\n",
    "\n",
    "# Read the 3dm file\n",
    "with open(input_3dm_file, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('ND'):\n",
    "            parts = line.split()\n",
    "            node_id = int(parts[1])\n",
    "            x = float(parts[2])\n",
    "            y = float(parts[3])\n",
    "            z = float(parts[4])\n",
    "            nodes.append([node_id, x, y, z])\n",
    "        elif line.startswith('E3T'):\n",
    "            parts = line.split()\n",
    "            elem_id = int(parts[1])\n",
    "            node1 = int(parts[2])\n",
    "            node2 = int(parts[3])\n",
    "            node3 = int(parts[4])\n",
    "            material_id = int(parts[5])\n",
    "            elements.append([elem_id, node1, node2, node3, material_id])\n",
    "\n",
    "# Track all unique time steps\n",
    "all_time_steps = set()\n",
    "\n",
    "# Process each data file\n",
    "for input_data_file in input_data_files:\n",
    "    metadata, data = parse_data_file(input_data_file)\n",
    "    file_base_name = os.path.splitext(os.path.basename(input_data_file))[0]\n",
    "    data_dicts[file_base_name] = data\n",
    "\n",
    "    # Extract time steps from data\n",
    "    time_steps = sorted(data.keys())\n",
    "    all_time_steps.update(time_steps)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "nodes = np.array(nodes)\n",
    "elements = np.array(elements)\n",
    "all_time_steps = sorted(all_time_steps)\n",
    "\n",
    "# Write data to HDF5 file\n",
    "with h5py.File(output_h5_file, 'w') as h5_file:\n",
    "    h5_file.create_dataset('nodes', data=nodes[:, 1:])  # Store x, y, z coordinates\n",
    "    h5_file.create_dataset('elements', data=elements[:, 1:4] - 1)  # Store node connectivity (0-based indexing)\n",
    "    h5_file.create_dataset('material_ids', data=elements[:, 4])  # Store material IDs\n",
    "    for file_base_name, data in data_dicts.items():\n",
    "        for ts in all_time_steps:\n",
    "            if ts in data:\n",
    "                data_ts = np.array(data[ts])\n",
    "            else:\n",
    "                # If the timestep is missing in the current dataset, use a zero array\n",
    "                data_ts = np.zeros((nodes.shape[0], len(data[next(iter(data))][0])))\n",
    "            h5_file.create_dataset(f'{file_base_name}/ts_{ts}', data=data_ts)  # Store data for each timestep\n",
    "\n",
    "# Create the Xdmf file compatible with Xdmf3ReaderS\n",
    "with open(output_xdmf_file, 'w') as xdmf_file:\n",
    "    xdmf_file.write(f'''<?xml version=\"1.0\" ?>\n",
    "<Xdmf Version=\"3.0\">\n",
    "  <Domain>\n",
    "    <Grid Name=\"Mesh\" GridType=\"Collection\" CollectionType=\"Temporal\">\n",
    "''')\n",
    "\n",
    "    # Write data for each timestep\n",
    "    for ts in all_time_steps:\n",
    "        xdmf_file.write(f'''      <Grid Name=\"Timestep_{ts}\" GridType=\"Uniform\">\n",
    "        <Time Value=\"{ts}\" />\n",
    "        <Topology TopologyType=\"Triangle\" NumberOfElements=\"{len(elements)}\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(elements)} 3\">\n",
    "            {output_h5_file}:/elements\n",
    "          </DataItem>\n",
    "        </Topology>\n",
    "        <Geometry GeometryType=\"XYZ\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(nodes)} 3\">\n",
    "            {output_h5_file}:/nodes\n",
    "          </DataItem>\n",
    "        </Geometry>\n",
    "        <Attribute Name=\"Material ID\" AttributeType=\"Scalar\" Center=\"Cell\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(elements)}\">\n",
    "            {output_h5_file}:/material_ids\n",
    "          </DataItem>\n",
    "        </Attribute>\n",
    "''')\n",
    "        for file_base_name in data_dicts.keys():\n",
    "            # Determine data type based on the first data entry length\n",
    "            sample_data = data_dicts[file_base_name][next(iter(data_dicts[file_base_name]))]\n",
    "            data_type = \"Vector\" if len(sample_data[0]) > 1 else \"Scalar\"\n",
    "            xdmf_file.write(f'''        <Attribute Name=\"{file_base_name}\" AttributeType=\"{data_type}\" Center=\"Node\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{nodes.shape[0]} {len(sample_data[0])}\" NumberType=\"Float\" Precision=\"8\">\n",
    "            {output_h5_file}:/{file_base_name}/ts_{ts}\n",
    "          </DataItem>\n",
    "        </Attribute>\n",
    "''')\n",
    "        xdmf_file.write('      </Grid>\\n')\n",
    "\n",
    "    xdmf_file.write('''    </Grid>\n",
    "  </Domain>\n",
    "</Xdmf>\n",
    "''')\n",
    "\n",
    "print(\"Conversion complete! HDF5 and XDMF files including data have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b19b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! HDF5 and XDMF files including data have been created.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to parse the 3dm file for nodes and elements\n",
    "def parse_3dm_file(file_path):\n",
    "    nodes = []\n",
    "    elements = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('ND'):\n",
    "                parts = line.split()\n",
    "                node_id = int(parts[1])\n",
    "                x = float(parts[2])\n",
    "                y = float(parts[3])\n",
    "                z = float(parts[4])\n",
    "                nodes.append([node_id, x, y, z])\n",
    "            elif line.startswith('E3T'):\n",
    "                parts = line.split()\n",
    "                elem_id = int(parts[1])\n",
    "                node1 = int(parts[2])\n",
    "                node2 = int(parts[3])\n",
    "                node3 = int(parts[4])\n",
    "                material_id = int(parts[5])\n",
    "                elements.append([elem_id, node1, node2, node3, material_id])\n",
    "    \n",
    "    return np.array(nodes), np.array(elements)\n",
    "\n",
    "# Function to parse the data file\n",
    "def parse_data_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    metadata = {}\n",
    "    data = {}\n",
    "    current_ts = None\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if not parts:\n",
    "            continue\n",
    "        if parts[0] == 'TS':\n",
    "            current_ts = float(parts[2])\n",
    "            data[current_ts] = []\n",
    "        elif parts[0] == 'ENDDS':\n",
    "            continue\n",
    "        elif current_ts is not None:\n",
    "            data[current_ts].append([float(p) for p in parts])\n",
    "    \n",
    "    return metadata, data\n",
    "\n",
    "# Function to parse depth data file\n",
    "def parse_depth_file(depth_file_path):\n",
    "    with open(depth_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    metadata = {}\n",
    "    data = {}\n",
    "    current_ts = None\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if not parts:\n",
    "            continue\n",
    "        if parts[0] == 'TS':\n",
    "            current_ts = float(parts[2])\n",
    "            data[current_ts] = []\n",
    "        elif parts[0] == 'ENDDS':\n",
    "            continue\n",
    "        elif current_ts is not None:\n",
    "            data[current_ts].append(float(parts[0]))\n",
    "    \n",
    "    return metadata, data\n",
    "\n",
    "# File paths\n",
    "name = \"Flap_Conc\"\n",
    "input_3dm_file = name + '.3dm'\n",
    "output_h5_file = name + '_wse.h5'\n",
    "output_xdmf_file = name + '_wse.xdmf'\n",
    "depth_file = 'Flap_Conc_dep.dat'  # Replace with your actual depth data file path\n",
    "input_data_files = [depth_file]  # Replace with your actual data file paths\n",
    "\n",
    "# Parse 3dm file for nodes and elements\n",
    "nodes, elements = parse_3dm_file(input_3dm_file)\n",
    "\n",
    "# Parse depth data file\n",
    "depth_metadata, depth_data = parse_depth_file(depth_file)\n",
    "\n",
    "# Track all unique time steps\n",
    "all_time_steps = set()\n",
    "\n",
    "# Process each data file\n",
    "data_dicts = {}\n",
    "for input_data_file in input_data_files:\n",
    "    metadata, data = parse_data_file(input_data_file)\n",
    "    file_base_name = os.path.splitext(os.path.basename(input_data_file))[0]\n",
    "    data_dicts[file_base_name] = data\n",
    "\n",
    "    # Extract time steps from data\n",
    "    time_steps = sorted(data.keys())\n",
    "    all_time_steps.update(time_steps)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "nodes = np.array(nodes)\n",
    "elements = np.array(elements)\n",
    "all_time_steps = sorted(all_time_steps)\n",
    "\n",
    "# Write data to HDF5 file\n",
    "with h5py.File(output_h5_file, 'w') as h5_file:\n",
    "    h5_file.create_dataset('elements', data=elements[:, 1:4] - 1)  # Store element connectivity (0-based indexing)\n",
    "    h5_file.create_dataset('material_ids', data=elements[:, 4])  # Store material IDs\n",
    "\n",
    "    for ts in all_time_steps:\n",
    "        # Modify node z-coordinates with depth information for each timestep\n",
    "        nodes_ts = nodes.copy()\n",
    "        if ts in depth_data:\n",
    "            depth_value = depth_data[ts][0]  # Assuming depth data is a scalar per timestep\n",
    "            nodes_ts[:, 3] += depth_value  # Add depth value to node z-coordinate\n",
    "        \n",
    "        # Store nodes for each timestep as a dataset\n",
    "        h5_file.create_dataset(f'nodes/ts_{ts}', data=nodes_ts[:, 1:])  # Store x, y, z coordinates\n",
    "        \n",
    "\n",
    "\n",
    "# Create the Xdmf file compatible with Xdmf3ReaderS\n",
    "with open(output_xdmf_file, 'w') as xdmf_file:\n",
    "    xdmf_file.write(f'''<?xml version=\"1.0\" ?>\n",
    "<Xdmf Version=\"3.0\">\n",
    "  <Domain>\n",
    "    <Grid Name=\"Mesh\" GridType=\"Collection\" CollectionType=\"Temporal\">\n",
    "''')\n",
    "\n",
    "    for ts in all_time_steps:\n",
    "        xdmf_file.write(f'''      <Grid Name=\"Timestep_{ts}\" GridType=\"Uniform\">\n",
    "        <Time Value=\"{ts}\" />\n",
    "        <Topology TopologyType=\"Triangle\" NumberOfElements=\"{len(elements)}\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(elements)} 3\">\n",
    "            {output_h5_file}:/elements\n",
    "          </DataItem>\n",
    "        </Topology>\n",
    "        <Geometry GeometryType=\"XYZ\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{nodes.shape[0]} 3\">\n",
    "            {output_h5_file}:/nodes/ts_{ts}\n",
    "          </DataItem>\n",
    "        </Geometry>\n",
    "        <Attribute Name=\"Material ID\" AttributeType=\"Scalar\" Center=\"Cell\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(elements)}\">\n",
    "            {output_h5_file}:/material_ids\n",
    "          </DataItem>\n",
    "        </Attribute>\n",
    "''')\n",
    "        xdmf_file.write('      </Grid>\\n')\n",
    "\n",
    "    xdmf_file.write('''    </Grid>\n",
    "  </Domain>\n",
    "</Xdmf>\n",
    "''')\n",
    "\n",
    "print(\"Conversion complete! HDF5 and XDMF files including data have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a162da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
