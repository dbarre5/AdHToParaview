{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb7162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3dd8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "7250.0\n",
      "14437.5\n",
      "21781.25\n",
      "28843.75\n",
      "36015.625\n",
      "43312.5\n",
      "50562.5\n",
      "57812.5\n",
      "65000.0\n",
      "72000.0\n",
      "79250.0\n",
      "86500.0\n",
      "93687.5\n",
      "100937.5\n",
      "108187.5\n",
      "115375.0\n",
      "122625.0\n",
      "129625.0\n",
      "137046.875\n",
      "144046.875\n",
      "151296.875\n",
      "Conversion complete! HDF5 and XDMF files including data have been created.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "name = \"MTOG_ADH_FINAL\"\n",
    "input_3dm_file = name + '.3dm'\n",
    "output_h5_file = name + '.h5'\n",
    "output_xdmf_file = name + '.xdmf'\n",
    "input_data_files = ['MTOG_ADH_FINAL_ovl.dat', 'MTOG_ADH_FINAL_dep.dat', 'MTOG_ADH_FINAL_con1.dat']  # Replace with your actual data file paths\n",
    "\n",
    "# Define the time range if needed\n",
    "start_time = 0.0  # Replace with start time if filtering by range, Use NONE for whole range\n",
    "end_time = 144046.875    # Replace with end time if filtering by range, Use NONE for whole range\n",
    "\n",
    "def parse_data_file(file_path, start_time=None, end_time=None):\n",
    "    metadata = {}\n",
    "    data = {}\n",
    "    current_ts = None\n",
    "    first_valid_ts = None  # Track the first valid timestamp >= start_time\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            if parts[0] == 'ND':\n",
    "                metadata['ND'] = int(parts[1])\n",
    "            elif parts[0] == 'NC':\n",
    "                metadata['NC'] = int(parts[1])\n",
    "            elif parts[0] == 'NAME':\n",
    "                metadata['NAME'] = parts[1]\n",
    "            elif parts[0] == 'TIMEUNITS':\n",
    "                metadata['TIMEUNITS'] = parts[1]\n",
    "            elif parts[0] == 'TS':\n",
    "                current_ts = float(parts[2])\n",
    "                print(current_ts)\n",
    "                if (start_time is None or current_ts >= start_time) and (end_time is None or current_ts <= end_time):\n",
    "                    data[current_ts] = []\n",
    "                    if first_valid_ts is None:\n",
    "                        first_valid_ts = current_ts\n",
    "                elif current_ts > end_time:\n",
    "                    break  # Stop parsing TS lines if current_ts exceeds end_time\n",
    "            elif parts[0] == 'ENDDS':\n",
    "                if first_valid_ts is not None and current_ts is None:\n",
    "                    break  # Stop parsing further if we've passed end time without finding any valid TS lines\n",
    "            elif current_ts is not None:\n",
    "                if current_ts in data:  # Ensure we only append data if current_ts is in the filtered range\n",
    "                    data[current_ts].append([float(p) for p in parts])\n",
    "    \n",
    "    return metadata, data, first_valid_ts\n",
    "\n",
    "\n",
    "# Initialize lists and dictionaries to store nodes, elements, and data\n",
    "nodes = []\n",
    "elements = []\n",
    "data_dicts = {}  # Dictionary to store data lists for each timestep keyed by file base name\n",
    "\n",
    "# Read the 3dm file to extract nodes and elements\n",
    "with open(input_3dm_file, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('ND'):\n",
    "            parts = line.split()\n",
    "            node_id = int(parts[1])\n",
    "            x = float(parts[2])\n",
    "            y = float(parts[3])\n",
    "            z = float(parts[4])\n",
    "            nodes.append([node_id, x, y, z])\n",
    "        elif line.startswith('E3T'):\n",
    "            parts = line.split()\n",
    "            elem_id = int(parts[1])\n",
    "            node1 = int(parts[2])\n",
    "            node2 = int(parts[3])\n",
    "            node3 = int(parts[4])\n",
    "            material_id = int(parts[5])\n",
    "            elements.append([elem_id, node1, node2, node3, material_id])\n",
    "\n",
    "# Convert lists to numpy arrays for efficiency\n",
    "nodes = np.array(nodes)\n",
    "elements = np.array(elements)\n",
    "\n",
    "# Track all unique time steps across all data files\n",
    "all_time_steps = set()\n",
    "\n",
    "# Process each data file to parse metadata and filtered data\n",
    "for input_data_file in input_data_files:\n",
    "    # Assuming here that data files are sorted by time steps and we're filtering based on a specified range\n",
    "    metadata, data, first_valid_ts = parse_data_file(input_data_file, start_time=start_time, end_time=end_time)\n",
    "    file_base_name = os.path.splitext(os.path.basename(input_data_file))[0]\n",
    "    data_dicts[file_base_name] = data\n",
    "    \n",
    "    # Add time steps from current data file to all_time_steps\n",
    "    if first_valid_ts is not None:\n",
    "        all_time_steps.add(first_valid_ts)\n",
    "        all_time_steps.update(data.keys())\n",
    "\n",
    "# Filter time steps to include only those within the specified range\n",
    "if start_time is not None and end_time is not None:\n",
    "    filtered_time_steps = sorted(filter(lambda ts: start_time <= ts <= end_time, all_time_steps))\n",
    "else:\n",
    "    filtered_time_steps = sorted(all_time_steps)  # If no range specified, use all time steps\n",
    "\n",
    "# Write data to HDF5 file\n",
    "with h5py.File(output_h5_file, 'w') as h5_file:\n",
    "    # Store nodes, elements, and material_ids\n",
    "    h5_file.create_dataset('nodes', data=nodes[:, 1:])  # Store x, y, z coordinates\n",
    "    h5_file.create_dataset('elements', data=elements[:, 1:4] - 1)  # Store node connectivity (0-based indexing)\n",
    "    h5_file.create_dataset('material_ids', data=elements[:, 4])  # Store material IDs\n",
    "    \n",
    "    # Store data for each file and each timestep within the filtered range\n",
    "    for file_base_name, data in data_dicts.items():\n",
    "        for ts in filtered_time_steps:\n",
    "            if ts in data:\n",
    "                data_ts = np.array(data[ts])\n",
    "            else:\n",
    "                data_ts = np.zeros((nodes.shape[0], len(data[next(iter(data))][0])))  # Zero array if timestep not present\n",
    "            h5_file.create_dataset(f'{file_base_name}/ts_{ts}', data=data_ts)\n",
    "\n",
    "# Write XDMF file\n",
    "with open(output_xdmf_file, 'w') as xdmf_file:\n",
    "    xdmf_file.write(f'''<?xml version=\"1.0\" ?>\n",
    "<Xdmf Version=\"3.0\">\n",
    "  <Domain>\n",
    "    <Grid Name=\"Mesh\" GridType=\"Collection\" CollectionType=\"Temporal\">\n",
    "''')\n",
    "\n",
    "    # Write data for each timestep within the filtered range\n",
    "    for ts in filtered_time_steps:\n",
    "        xdmf_file.write(f'''      <Grid Name=\"Timestep_{ts}\" GridType=\"Uniform\">\n",
    "        <Time Value=\"{ts}\" />\n",
    "        <Topology TopologyType=\"Triangle\" NumberOfElements=\"{len(elements)}\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(elements)} 3\">\n",
    "            {output_h5_file}:/elements\n",
    "          </DataItem>\n",
    "        </Topology>\n",
    "        <Geometry GeometryType=\"XYZ\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(nodes)} 3\">\n",
    "            {output_h5_file}:/nodes\n",
    "          </DataItem>\n",
    "        </Geometry>\n",
    "        <Attribute Name=\"Material ID\" AttributeType=\"Scalar\" Center=\"Cell\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(elements)}\">\n",
    "            {output_h5_file}:/material_ids\n",
    "          </DataItem>\n",
    "        </Attribute>\n",
    "''')\n",
    "        for file_base_name in data_dicts.keys():\n",
    "            sample_data = data_dicts[file_base_name][next(iter(data_dicts[file_base_name]))]\n",
    "            data_type = \"Vector\" if len(sample_data[0]) > 1 else \"Scalar\"\n",
    "            xdmf_file.write(f'''        <Attribute Name=\"{file_base_name}\" AttributeType=\"{data_type}\" Center=\"Node\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{nodes.shape[0]} {len(sample_data[0])}\" NumberType=\"Float\" Precision=\"8\">\n",
    "            {output_h5_file}:/{file_base_name}/ts_{ts}\n",
    "          </DataItem>\n",
    "        </Attribute>\n",
    "''')\n",
    "        xdmf_file.write('      </Grid>\\n')\n",
    "\n",
    "    xdmf_file.write('''    </Grid>\n",
    "  </Domain>\n",
    "</Xdmf>\n",
    "''')\n",
    "\n",
    "print(\"Conversion complete! HDF5 and XDMF files including data have been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef2c8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! HDF5 and XDMF files including data have been created.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "name = \"MTOG_ADH_FINAL\"\n",
    "input_3dm_file = name + '.3dm'\n",
    "output_h5_file = name + '_wse.h5'\n",
    "output_xdmf_file = name + '_wse.xdmf'\n",
    "depth_file = 'MTOG_ADH_FINAL_dep.dat'  # Replace with your actual depth data file path\n",
    "\n",
    "# Define start_time and end_time for filtering\n",
    "start_time = 0.0  # Replace with start time if filtering by range, Use NONE for whole range\n",
    "end_time = 444046.875  # Replace with end time if filtering by range, Use NONE for whole range\n",
    "\n",
    "# Function to parse the 3dm file for nodes and elements\n",
    "def parse_3dm_file(file_path):\n",
    "    nodes = []\n",
    "    elements = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('ND'):\n",
    "                parts = line.split()\n",
    "                node_id = int(parts[1])\n",
    "                x = float(parts[2])\n",
    "                y = float(parts[3])\n",
    "                z = float(parts[4])\n",
    "                nodes.append([node_id, x, y, z])\n",
    "            elif line.startswith('E3T'):\n",
    "                parts = line.split()\n",
    "                elem_id = int(parts[1])\n",
    "                node1 = int(parts[2])\n",
    "                node2 = int(parts[3])\n",
    "                node3 = int(parts[4])\n",
    "                material_id = int(parts[5])\n",
    "                elements.append([elem_id, node1, node2, node3, material_id])\n",
    "    \n",
    "    return np.array(nodes), np.array(elements)\n",
    "\n",
    "# Function to parse the depth file for timestamps and depth data\n",
    "def parse_depth_file(depth_file_path, start_time=None, end_time=None):\n",
    "    metadata = {}\n",
    "    data = {}\n",
    "    current_ts = None\n",
    "    first_valid_ts = None  # Track the first valid timestamp >= start_time\n",
    "    \n",
    "    with open(depth_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            if parts[0] == 'ND':\n",
    "                metadata['ND'] = int(parts[1])\n",
    "            elif parts[0] == 'NC':\n",
    "                metadata['NC'] = int(parts[1])\n",
    "            elif parts[0] == 'NAME':\n",
    "                metadata['NAME'] = parts[1]\n",
    "            elif parts[0] == 'TIMEUNITS':\n",
    "                metadata['TIMEUNITS'] = parts[1]\n",
    "            elif parts[0] == 'TS':\n",
    "                current_ts = float(parts[2])\n",
    "                if (start_time is None or current_ts >= start_time) and (end_time is None or current_ts <= end_time):\n",
    "                    data[current_ts] = []\n",
    "                    if first_valid_ts is None:\n",
    "                        first_valid_ts = current_ts\n",
    "                elif current_ts > end_time:\n",
    "                    break  # Stop parsing TS lines if current_ts exceeds end_time\n",
    "            elif parts[0] == 'ENDDS':\n",
    "                if first_valid_ts is not None and current_ts is None:\n",
    "                    break  # Stop parsing further if we've passed end time without finding any valid TS lines\n",
    "            elif current_ts is not None:\n",
    "                if current_ts in data:  # Ensure we only append data if current_ts is in the filtered range\n",
    "                    data[current_ts].append(float(parts[0]))\n",
    "    \n",
    "    return metadata, data, first_valid_ts\n",
    "\n",
    "# Parse 3dm file for nodes and elements\n",
    "nodes, elements = parse_3dm_file(input_3dm_file)\n",
    "\n",
    "# Parse depth data file\n",
    "depth_metadata, depth_data, all_time_steps = parse_depth_file(depth_file, start_time=start_time, end_time=end_time)\n",
    "\n",
    "# Convert nodes and elements to numpy arrays\n",
    "nodes = np.array(nodes)\n",
    "elements = np.array(elements)\n",
    "\n",
    "# Write data to HDF5 file\n",
    "with h5py.File(output_h5_file, 'w') as h5_file:\n",
    "    # Store elements and material IDs\n",
    "    h5_file.create_dataset('elements', data=elements[:, 1:4] - 1)  # Store element connectivity (0-based indexing)\n",
    "    h5_file.create_dataset('material_ids', data=elements[:, 4])  # Store material IDs\n",
    "\n",
    "    # Store nodes with depth values for each timestep and depth as a separate attribute\n",
    "    for ts, depth_value in depth_data.items():\n",
    "        nodes_ts = nodes.copy()  # Copy nodes array\n",
    "        nodes_ts[:, 3] += depth_value  # Add depth values to z-coordinate\n",
    "        h5_file.create_dataset(f'nodes/ts_{ts}', data=nodes_ts[:, 1:])  # Store x, y, z coordinates\n",
    "        h5_file.create_dataset(f'depth/ts_{ts}', data=depth_value)  # Store depth values separately\n",
    "\n",
    "# Write XDMF file\n",
    "with open(output_xdmf_file, 'w') as xdmf_file:\n",
    "    xdmf_file.write(f'''<?xml version=\"1.0\" ?>\n",
    "<Xdmf Version=\"3.0\">\n",
    "  <Domain>\n",
    "    <Grid Name=\"Mesh\" GridType=\"Collection\" CollectionType=\"Temporal\">\n",
    "''')\n",
    "\n",
    "    for ts, depth_value in depth_data.items():\n",
    "        xdmf_file.write(f'''      <Grid Name=\"Timestep_{ts}\" GridType=\"Uniform\">\n",
    "        <Time Value=\"{ts}\" />\n",
    "        <Topology TopologyType=\"Triangle\" NumberOfElements=\"{len(elements)}\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(elements)} 3\">\n",
    "            {output_h5_file}:/elements\n",
    "          </DataItem>\n",
    "        </Topology>\n",
    "        <Geometry GeometryType=\"XYZ\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{nodes.shape[0]} 3\">\n",
    "            {output_h5_file}:/nodes/ts_{ts}\n",
    "          </DataItem>\n",
    "        </Geometry>\n",
    "        <Attribute Name=\"Material ID\" AttributeType=\"Scalar\" Center=\"Cell\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{len(elements)}\">\n",
    "            {output_h5_file}:/material_ids\n",
    "          </DataItem>\n",
    "        </Attribute>\n",
    "        <Attribute Name=\"Depth\" AttributeType=\"Scalar\" Center=\"Node\">\n",
    "          <DataItem Format=\"HDF\" Dimensions=\"{nodes.shape[0]}\">\n",
    "            {output_h5_file}:/depth/ts_{ts}\n",
    "          </DataItem>\n",
    "        </Attribute>\n",
    "''')\n",
    "        xdmf_file.write('      </Grid>\\n')\n",
    "\n",
    "    xdmf_file.write('''    </Grid>\n",
    "  </Domain>\n",
    "</Xdmf>\n",
    "''')\n",
    "\n",
    "print(\"Conversion complete! HDF5 and XDMF files including data have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data[0][931838]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8a7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
